{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3IYjcbStHat"
      },
      "source": [
        "### 라이브러리 설치 및 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XL5b-Heo5vv",
        "outputId": "c9071c65-544d-478d-f7cf-c88c677f50ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SpupJ1oequeg",
        "outputId": "b918fdb3-164d-4c4c-976a-9e65d7fdebfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1jGRh_AZCxmA",
        "outputId": "654324b1-1005-4a9c-e1c8-4b21d128352f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "GsDQAtd35Kih",
        "outputId": "bad1a4e0-a372-42ce-aedc-f850b8c9f401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "5fcbed3506614c24b548c6f7d698ea5c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgnAvg-Ns-_7",
        "outputId": "1e3484b2-54ad-4fb5-9e47-b7faf0a2bd23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed transformers-4.57.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Y-djroopb_",
        "outputId": "48964a76-337b-4e6c-faa0-7ce62bb9689b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting BerTopic\n",
            "  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (0.8.40)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (0.5.9.post2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (5.1.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (4.67.1)\n",
            "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.12/dist-packages (from BerTopic) (0.43.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->BerTopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->BerTopic) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->BerTopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->BerTopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->BerTopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->BerTopic) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->BerTopic) (25.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->BerTopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->BerTopic) (4.57.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->BerTopic) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->BerTopic) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->BerTopic) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->BerTopic) (4.15.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->BerTopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->BerTopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->BerTopic) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->BerTopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->BerTopic) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->BerTopic) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->BerTopic) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->BerTopic) (2025.10.5)\n",
            "Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: BerTopic\n",
            "Successfully installed BerTopic-0.17.3\n"
          ]
        }
      ],
      "source": [
        "!pip install BerTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICBJuEUSfhKK",
        "outputId": "2bc3627f-8d90-4526-e487-aa953613b5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import requests, xml.etree.ElementTree as ET, json\n",
        "from urllib import parse\n",
        "import re, datetime, traceback\n",
        "from Cryptodome.Cipher import AES # type: ignore\n",
        "from urllib.parse import quote\n",
        "import base64\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import requests\n",
        "\n",
        "from gensim import corpora, models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB62KY2otMJ9"
      },
      "source": [
        "### 경로 및 API 정보 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeVB1SHMoVva"
      },
      "outputs": [],
      "source": [
        "# CSV 파일 경로\n",
        "DATA_DIR = \"/content/drive/MyDrive/NexaLab\"\n",
        "\n",
        "#사용자 정보 입력\n",
        "DataON_KEY = \"your_dataon_key\"\n",
        "\n",
        "ScienceON_MAC_address = \"your_mac_address\"\n",
        "ScienceON_clientID = \"your_client_id\"\n",
        "ScienceON_KEY = \"your_scienceon_key\"\n",
        "\n",
        "# Qwen API\n",
        "url = \"https://aida.kisti.re.kr:10414/v1/chat/completions\"  # Qwen3-14B OpenAI 호환 엔드포인트\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbnZzIjp7fSwicm91dGVfaW5mbyI6W3sicm91dGVfaWQiOiJlMjkxNDI3Ni1kMDVlLTQ5Y2ItODFjYy1mOTAwZDA5Y2VkMWQiLCJzZXNzaW9uX2lkIjoiMDE2NDA2N2QtZTRlNS00YzM1LWJjZDgtMWU2YzRiYzY2YWQ5Iiwic2Vzc2lvbl9uYW1lIjpudWxsLCJrZXJuZWxfaG9zdCI6IjE1MC4xODMuMTI4LjI4Iiwia2VybmVsX3BvcnQiOjMwMDY3LCJwcm90b2NvbCI6Imh0dHAiLCJ0cmFmZmljX3JhdGlvIjoxLjB9LHsicm91dGVfaWQiOiI5NDI2NmVkNC04ZmQwLTRhOWEtYTEwZS1jMzdjNTIzMTBiNjkiLCJzZXNzaW9uX2lkIjoiOGNmNTRhMDQtOTA4Yy00ODdkLTliNjAtNmI3YzE3YzAyZGVkIiwic2Vzc2lvbl9uYW1lIjpudWxsLCJrZXJuZWxfaG9zdCI6IjE1MC4xODMuMTI4LjI3Iiwia2VybmVsX3BvcnQiOjMwOTY1LCJwcm90b2NvbCI6Imh0dHAiLCJ0cmFmZmljX3JhdGlvIjoxLjB9LHsicm91dGVfaWQiOiJkNDIwNzEyOC0yZWU5LTQwMDEtOWRjZC0wOGZlMjIzOGEyYjgiLCJzZXNzaW9uX2lkIjoiOGZlNTRlMWMtMDkxYi00ZDY5LWI1YzUtN2M5YTk0MjZkNDM1Iiwic2Vzc2lvbl9uYW1lIjpudWxsLCJrZXJuZWxfaG9zdCI6IjE1MC4xODMuMTI4LjI3Iiwia2VybmVsX3BvcnQiOjMwOTc2LCJwcm90b2NvbCI6Imh0dHAiLCJ0cmFmZmljX3JhdGlvIjoxLjB9XSwiYXJndW1lbnRzIjpudWxsLCJjcmVhdGVkX2F0IjoxNzU0ODk4ODg2LjA1ODc3NywiYXBwIjoicXdlbjMtMTRiLTgwMDEiLCJvcGVuX3RvX3B1YmxpYyI6ZmFsc2UsInVwZGF0ZWRfYXQiOjE3NTQ4OTg4ODYuMDU4ODA0LCJ3b3JrZXIiOiJkOTFmYTRkMC03NjljLTQyMWUtOTAzMi0xMjljZGQ1ZjhhMDgiLCJhbGxvd2VkX2NsaWVudF9pcHMiOm51bGwsImlkIjoiMzlkMTBmNjEtYmJlYy00NzExLWFjOGEtODJmNWZlMTU0YTE0IiwiYXBwX21vZGUiOiJpbmZlcmVuY2UiLCJ1c2VyX2lkIjoiNDVhNzYxYmUtNzczNC00MWQxLTk0YTYtMTI2ZmI3ZDQwYThmIiwicHJvdG9jb2wiOiJodHRwIiwiZnJvbnRlbmRfbW9kZSI6InBvcnQiLCJlbmRwb2ludF9pZCI6ImQ2OGQ4NTUyLTNiMDktNDBjZS1iZjJhLTA2MDI4M2UwYzJkMSIsInBvcnQiOjEwNDE0LCJydW50aW1lX3ZhcmlhbnQiOiJjdXN0b20iLCJzdWJkb21haW4iOm51bGwsInNlc3Npb25faWRzIjpbIjAxNjQwNjdkLWU0ZTUtNGMzNS1iY2Q4LTFlNmM0YmM2NmFkOSIsIjhjZjU0YTA0LTkwOGMtNDg3ZC05YjYwLTZiN2MxN2MwMmRlZCIsIjhmZTU0ZTFjLTA5MWItNGQ2OS1iNWM1LTdjOWE5NDI2ZDQzNSJdLCJjb25maWciOnt9LCJhcHBfdXJsIjoiaHR0cHM6Ly9haWRhLmtpc3RpLnJlLmtyOjEwNDE0IiwidXNlciI6IjQ1YTc2MWJlLTc3MzQtNDFkMS05NGE2LTEyNmZiN2Q0MGE4ZiIsImV4cCI6MTc2MTkyMjc5OX0.bezOhXJMTx21xa3NQL2GZom6dotPY8v-c3aG4Of8qeQ\",  # 여기에 실제 API 키 입력\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbC5CWg_tQKd"
      },
      "source": [
        "### API 호출 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8XN6V__6Hgq"
      },
      "outputs": [],
      "source": [
        "def request_dataon(search_query, row_count=1):\n",
        "    BASE_SEARCH_URL = \"https://dataon.kisti.re.kr/rest/api/search/dataset\"\n",
        "\n",
        "    all_results = []\n",
        "    params = {\n",
        "        \"key\": DataON_KEY,\n",
        "        \"query\": search_query,\n",
        "        \"from\": 0,\n",
        "        \"size\": row_count\n",
        "    }\n",
        "    response = requests.get(BASE_SEARCH_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        records = data.get(\"records\", [])\n",
        "        if not records:\n",
        "            print(f\"No records found for '{search_query}'\")\n",
        "\n",
        "        for rec in records:\n",
        "            all_results.append({\n",
        "                \"query\": search_query,\n",
        "                \"svc_id\": rec.get(\"svc_id\", \"\"),\n",
        "                \"mnsb_pc\" : rec.get(\"dataset_mnsb_pc\", \"\"),\n",
        "                \"title\" : rec.get(\"dataset_title_etc_main\"),\n",
        "                \"description\": rec.get(\"dataset_expl_etc_main\"),\n",
        "                \"keyword\": rec.get(\"dataset_kywd_etc_main\"),\n",
        "                \"creator\": rec.get(\"dataset_creator_etc_sub\", \"\"),\n",
        "                \"publisher\": rec.get(\"cltfm_etc\", \"\"),\n",
        "                \"year\": rec.get(\"dataset_pub_dt_pc\", \"\"),\n",
        "                \"public\": rec.get(\"dataset_access_type_pc\", \"\"),\n",
        "                \"url\": rec.get(\"dataset_lndgpg\", \"\")  # 데이터 제공처 링크\n",
        "            })\n",
        "    else:\n",
        "        print(f\"Error searching '{search_query}': {response.status_code}, {response.text}\")\n",
        "\n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jvYlK5VpA80"
      },
      "outputs": [],
      "source": [
        "class AESTestClass:\n",
        "    def __init__(self, plain_txt, key):\n",
        "        # iv, block_size 값은 고정입니다.\n",
        "        self.iv = 'jvHJ1EFA0IXBrxxz'\n",
        "        self.block_size = 16\n",
        "        self.plain_txt = plain_txt\n",
        "        self.key = key\n",
        "\n",
        "    def pad(self):\n",
        "        number_of_bytes_to_pad = self.block_size - len(self.plain_txt) % self.block_size\n",
        "        ascii_str = chr(number_of_bytes_to_pad)\n",
        "        padding_str = number_of_bytes_to_pad * ascii_str\n",
        "        print(padding_str.encode('utf-8'))\n",
        "        padded_plain_text = self.plain_txt + padding_str\n",
        "        return padded_plain_text\n",
        "\n",
        "    def encrypt(self):\n",
        "        cipher = AES.new(self.key.encode('utf-8'), AES.MODE_CBC, self.iv.encode('utf-8'))\n",
        "        padded_txt = AESTestClass.pad(self)\n",
        "        encrypted_bytes = cipher.encrypt(padded_txt.encode('utf-8'))\n",
        "        encrypted_str = base64.urlsafe_b64encode(encrypted_bytes).decode(\"utf-8\")\n",
        "        return encrypted_str\n",
        "\n",
        "refreshToken = None\n",
        "accessToken = None\n",
        "\n",
        "def createToken():\n",
        "    global refreshToken, accessToken\n",
        "    try:\n",
        "        time = ''.join(re.findall(r\"\\d\", datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
        "        plain_txt = json.dumps({\"datetime\": time, \"mac_address\": ScienceON_MAC_address}).replace(\" \", \"\")\n",
        "        encryption = AESTestClass(plain_txt, ScienceON_KEY)\n",
        "        encrypted_txt = encryption.encrypt()\n",
        "\n",
        "        target_URL = f\"https://apigateway.kisti.re.kr/tokenrequest.do?client_id={ScienceON_clientID}&accounts={encrypted_txt}\"\n",
        "        response = requests.get(target_URL)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        json_object = response.json()\n",
        "        refreshToken = json_object.get('refresh_token')\n",
        "        accessToken = json_object.get('access_token')\n",
        "        print('새 Refresh Token과 Access Token 발급 완료')\n",
        "    except Exception:\n",
        "        traceback.print_exc()\n",
        "\n",
        "def getAccessToken():\n",
        "    global accessToken\n",
        "    try:\n",
        "        target_URL = f\"https://apigateway.kisti.re.kr/tokenrequest.do?refreshToken={refreshToken}&client_id={ScienceON_clientID}\"\n",
        "        response = requests.get(target_URL)\n",
        "        response.raise_for_status()\n",
        "        if 'errorCode' in response.text:\n",
        "            return None\n",
        "        json_object = response.json()\n",
        "        accessToken = json_object.get('access_token')\n",
        "        print('Access Token 재발급 완료')\n",
        "        return accessToken\n",
        "    except Exception:\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def request_scienceon(search_query, row_count=3, retry=0):\n",
        "    global accessToken\n",
        "    MAX_RETRY = 2\n",
        "\n",
        "    if not accessToken:\n",
        "        createToken()\n",
        "\n",
        "    query = {\"BI\": search_query}\n",
        "    search_query_encoded = parse.quote(json.dumps(query, ensure_ascii=False))\n",
        "    target_URL = (\n",
        "        f\"https://apigateway.kisti.re.kr/openapicall.do?\"\n",
        "        f\"client_id={ScienceON_clientID}&token={accessToken}&version=1.0&action=search&target=ARTI\"\n",
        "        f\"&searchQuery={search_query_encoded}&curPage=1&rowCount={row_count}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = requests.get(target_URL)\n",
        "        response.raise_for_status()\n",
        "    except Exception as e:\n",
        "        print(\"HTTP 요청 실패:\", e)\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        root = ET.fromstring(response.text)\n",
        "    except ET.ParseError:\n",
        "        print(\"XML 파싱 실패:\", response.text)\n",
        "        return None\n",
        "\n",
        "    status_elem = root.find('resultSummary/statusCode')\n",
        "    if status_elem is None:\n",
        "        print(\"응답에 statusCode 없음:\", response.text)\n",
        "        return None\n",
        "\n",
        "    statusCode = status_elem.text\n",
        "    if int(statusCode) != 200:\n",
        "        errorCode_elem = root.find('errorDetail/errorCode')\n",
        "        errorMessage_elem = root.find('errorDetail/errorMessage')\n",
        "        errorCode = errorCode_elem.text if errorCode_elem is not None else \"Unknown\"\n",
        "        errorMessage = errorMessage_elem.text if errorMessage_elem is not None else \"Unknown\"\n",
        "\n",
        "        if errorCode == 'E4103' and retry < MAX_RETRY:  # 토큰 만료\n",
        "            print(\"AccessToken 만료, 재발급 시도...\")\n",
        "            if not getAccessToken():\n",
        "                createToken()\n",
        "            return request_scienceon(search_query, row_count, retry + 1)\n",
        "        else:\n",
        "            print(f\"알 수 없는 오류 발생: {errorMessage}\")\n",
        "            return None\n",
        "\n",
        "    # XML → 리스트 변환\n",
        "    records = []\n",
        "    for record in root.findall(\".//record\"):\n",
        "        title_elem = record.find(\".//item[@metaCode='Title']\")\n",
        "        description_elem = record.find(\".//item[@metaCode='Abstract']\")\n",
        "        keyword_elem = record.find(\".//item[@metaCode='Keyword']\")\n",
        "        url_elem = record.find(\".//item[@metaCode='FulltextURL']\")\n",
        "\n",
        "        records.append({\n",
        "            \"title\": title_elem.text if title_elem is not None else \"\",\n",
        "            \"description\": description_elem.text if description_elem is not None else \"\",\n",
        "            \"keyword\": keyword_elem.text if keyword_elem is not None else \"\",\n",
        "            \"url\": url_elem.text if url_elem is not None else \"\"\n",
        "\n",
        "        })\n",
        "\n",
        "    return records\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1kdl7n5tYsH"
      },
      "source": [
        "### Input에 대한 API 요청 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjuLZtof3XAc"
      },
      "outputs": [],
      "source": [
        "def collect_data(query, category_api, row_count=50, return_type=\"str\"):\n",
        "    import re\n",
        "\n",
        "    # 데이터 가져오기\n",
        "    if category_api == \"DataOn\":\n",
        "        data = request_dataon(query, row_count=row_count)\n",
        "        desired_keys = [\"svc_id\", \"title\", \"mnsb_pc\", \"description\", \"keyword\", \"url\"]\n",
        "        division = \"dataset\"\n",
        "    elif category_api == \"ScienceOn\":\n",
        "        data = request_scienceon(query, row_count=row_count)\n",
        "        desired_keys = [\"title\", \"description\", \"keyword\", \"url\"]\n",
        "        division = \"paper\"\n",
        "    else:\n",
        "        print(f\"Unknown category_api: {category_api}\")\n",
        "        return []\n",
        "\n",
        "    if not data:\n",
        "        print(f\"'{query}' 검색 결과가 없습니다.\")\n",
        "        return []\n",
        "\n",
        "    processed_docs = []\n",
        "\n",
        "    for doc in data:\n",
        "        field_texts = []  # 문자열 합치기용\n",
        "        processed_doc = {}\n",
        "        for k in desired_keys:\n",
        "            text = doc.get(k, \"\") or \"\"\n",
        "\n",
        "            # 리스트이면 쉼표로 합치기\n",
        "            if isinstance(text, list):\n",
        "                text = \", \".join(filter(None, text))\n",
        "\n",
        "            # HTML 태그 제거\n",
        "            text = re.sub(r\"<[^>]*>\", \" \", text)\n",
        "\n",
        "            # \\r, \\n 제거\n",
        "            text = text.replace(\"\\\\r\", \" \").replace(\"\\\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
        "\n",
        "            # 특수문자 제거 (문자+숫자+공백+쉼표만 남김)\n",
        "            text = re.sub(r\"[^가-힣a-zA-Z0-9\\s,]\", \" \", text)\n",
        "\n",
        "            # 연속 공백 제거\n",
        "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "            processed_doc[k] = text\n",
        "            field_texts.append(text)\n",
        "\n",
        "             # division 추가\n",
        "            processed_doc[\"division\"] = division\n",
        "\n",
        "        # 최종 반환 형태 결정\n",
        "        if return_type == \"str\":\n",
        "            combined_text = \", \".join(f\"{k}: {text}\" for k, text in zip(desired_keys, field_texts))\n",
        "            processed_docs.append(combined_text)\n",
        "        else:\n",
        "            processed_docs.append(processed_doc)\n",
        "\n",
        "    return processed_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g-YJIZVtgoE"
      },
      "source": [
        "### 검색식 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWpPd3jztj1m"
      },
      "outputs": [],
      "source": [
        "def generate_query(input_data, url,headers):\n",
        "  # 요청 데이터\n",
        "  data = {\n",
        "      \"model\": \"qwen3-14b\",\n",
        "      \"messages\": [\n",
        "          {\"role\": \"system\",\"content\":f'''\n",
        "당신은 학술 정보 검색 전문가입니다. 사용자가 제공한 검색 키워드를 기반으로 국내외 학술 DB에서 최대한 많은 관련 자료를 찾을 수 있는 **확장형 Boolean 검색식**을 만들어야 합니다.\n",
        "\n",
        "조건:\n",
        "1. 핵심 개념과 동의어를 파악하고, {input_data[0]['keyword']}를 반영합니다.\n",
        "2. 너무 구체적이거나 제외 조건(-)은 가능한 한 제거하여 자료 누락을 방지합니다.\n",
        "3. 국내용 검색식(Korean)과 국제용 검색식(English)을 각각 만듭니다.\n",
        "4. Boolean 연산자 사용법:\n",
        "   - () : 높은 우선 순위\n",
        "   - 공백 : AND\n",
        "   - | : OR\n",
        "   - \"\" : 정확히 일치하는 문구 검색\n",
        "\n",
        "출력 형식(항상 2개):\n",
        "Korean : [검색식]\n",
        "English : [검색식]\n",
        "\n",
        "추가 지침:\n",
        "- AND, OR 문자를 사용하지 말고 검색 연산자 공백과 |을 사용하세요.\n",
        "- 동의어나 유사 표현을 | 로 묶습니다.\n",
        "- 핵심 개념을 중심으로 공백으로 연결합니다.\n",
        "- 불필요한 제외 조건은 제거하고, 너무 좁은 의미의 검색식은 확장합니다.\n",
        "- 자료가 없더라도 반드시 2개의 출력값을 제공합니다.\n",
        "- 연도를 나타내는 숫자는 제외합니다.\n",
        "- 불용어와 불필요한 단어는 제외하고 키워드 적극적으로 반영하여 생성합니다.\n",
        "- 무조건 ()는 최대 3개 이하만 사용하세요.\n",
        "\n",
        "예시:\n",
        "['(multi type model | multi-type model) (birth death | birth-death) (bayesian inference | bayesian phylogenetics)']\n",
        "'''\n",
        "          },\n",
        "          {\"role\": \"user\", \"content\": f\"\"\"\n",
        "                                          Title: {input_data[0]['title']}\n",
        "                                          Topic: {input_data[0]['mnsb_pc']}\n",
        "                                          Description: {input_data[0]['description']}\n",
        "                                          Keywords: {input_data[0]['keyword']}\n",
        "                                          url: {input_data[0]['url']}\n",
        "                                          \"\"\"\n",
        "          }\n",
        "\n",
        "      ],\n",
        "      \"max_tokens\": 1000,  # 충분히 길게 설정\n",
        "      \"temperature\": 0.7\n",
        "  }\n",
        "\n",
        "  # API 호출\n",
        "  response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "  # 결과 출력\n",
        "  if response.status_code == 200:\n",
        "      result = response.json()\n",
        "      assistant_text = result['choices'][0]['message']['content']\n",
        "\n",
        "      # 문자열에서 각 검색식 추출\n",
        "      lines = assistant_text.strip().split(\"\\n\")\n",
        "      korean_queries = [line.split(\":\", 1)[1].strip() for line in lines if line.startswith(\"Korean\")]\n",
        "      english_queries = [line.split(\":\", 1)[1].strip() for line in lines if line.startswith(\"English\")]\n",
        "\n",
        "      return korean_queries, english_queries\n",
        "\n",
        "  else:\n",
        "      print(\"Error:\", response.status_code, response.text) # 24초\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def limit_boolean_query(query, max_parens=3, max_or=5, max_quotes=2):\n",
        "    \"\"\"\n",
        "    검색식 제한 적용 (DB 안전용)\n",
        "    - 괄호 최대 max_parens\n",
        "    - 괄호 내부 OR 최대 max_or\n",
        "    - 따옴표(\"\") 최대 max_quotes\n",
        "    \"\"\"\n",
        "    quotes = re.findall(r'\"[^\"]*\"', query)\n",
        "    if len(quotes) > max_quotes:\n",
        "        for q in quotes[max_quotes:]:\n",
        "            query = query.replace(q, q.strip('\"'))\n",
        "\n",
        "    paren_groups = re.findall(r'\\([^\\(\\)]*\\)', query)\n",
        "    keep_groups = paren_groups[:max_parens]\n",
        "    overflow_groups = paren_groups[max_parens:]\n",
        "\n",
        "    for pg in keep_groups:\n",
        "        query = query.replace(pg, f\"__KEEP__{pg}__KEEP__\", 1)\n",
        "\n",
        "    for pg in overflow_groups:\n",
        "        inner = pg.strip('()')\n",
        "        query = query.replace(pg, inner, 1)\n",
        "\n",
        "    def limit_or_inside_pg(pg):\n",
        "        parts = pg.strip('()').split('|')\n",
        "        if len(parts) > max_or:\n",
        "            return '(' + '|'.join(parts[:max_or]) + ')'\n",
        "        return pg\n",
        "\n",
        "    query = re.sub(r'\\([^\\(\\)]*\\)', lambda m: limit_or_inside_pg(m.group(0)), query)\n",
        "\n",
        "    keep_parts = re.findall(r'__KEEP__\\([^\\(\\)]*\\)__KEEP__', query)\n",
        "    for i, part in enumerate(keep_parts):\n",
        "        keep_parts[i] = part.replace('__KEEP__', '')\n",
        "    if keep_parts:\n",
        "        and_part = ' AND '.join(keep_parts)\n",
        "        query = re.sub(r'__KEEP__\\([^\\(\\)]*\\)__KEEP__', '', query)\n",
        "        query = f\"{and_part} {query}\"\n",
        "\n",
        "    # 5️⃣ 괄호 제거 후 남은 OR 연결 구조 보정\n",
        "    query = re.sub(r'\\s+', ' ', query.strip())\n",
        "    query = query.replace('AND |', 'AND')  # 잘못된 구문 정리\n",
        "    query = query.replace('||', '|')\n",
        "\n",
        "    return query.strip()\n"
      ],
      "metadata": {
        "id": "OnuefTRavrQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Omm2R-ti1u"
      },
      "source": [
        "### 추천 사유 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF8Gsgl49na-"
      },
      "outputs": [],
      "source": [
        "def generate_reason_llm(reference_text, candidate_text, similarity_score, url, headers):\n",
        "      data = {\n",
        "        \"model\": \"qwen3-14b\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": f\"\"\"\n",
        "당신은 학술 데이터 및 논문 추천 시스템의 설명 생성자입니다.\n",
        "오직 최종 추천 이유 문장만 출력하십시오.\n",
        "\n",
        "두 문서의 의미적/주제적 유사도 점수는 {similarity_score:.3f}입니다.\n",
        "\n",
        "다음 기준으로 추천 이유를 생성하세요:\n",
        "1. 주제 유사성: 두 논문의 연구 주제, 연구 대상, 주요 개념을 비교하여 설명\n",
        "2. 연구 방법 유사성: 사용된 연구 방법론, 분석 기법, 데이터 처리 방식을 비교\n",
        "3. 적용 분야 유사성: 연구가 적용되는 분야, 활용 가능성, 문제 해결 영역 등을 명확히 서술\n",
        "\n",
        "출력 지침:\n",
        "- 문체: 공식적이고 자연스러운 한국어\n",
        "- 문장 수: 정확히 2~3문장\n",
        "- 표현: '~등'이나 모호한 단어 사용 금지\n",
        "- 형식: 반드시 아래 형태로 출력\n",
        "\"추천 이유: [주제 유사성]. [연구 방법 유사성]. [적용 분야 유사성].\"\n",
        "            \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"\n",
        "                                            기준 문서: {reference_text}\n",
        "                                            후보 문서: {candidate_text}\n",
        "                                        \"\"\"\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 800,\n",
        "        \"temperature\": 0.7\n",
        "      }\n",
        "\n",
        "      try:\n",
        "          response = requests.post(url, headers=headers, json=data)\n",
        "          resp_json = response.json()\n",
        "\n",
        "          # 디버깅 출력\n",
        "          if 'error' in resp_json:\n",
        "              print(f\"LLM 에러: {resp_json['error']}\")\n",
        "              return \"추천 이유 생성 실패\"\n",
        "\n",
        "          text = resp_json['choices'][0]['message']['content'].strip()\n",
        "          text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
        "\n",
        "          if not text:\n",
        "              print(\"빈 응답 받음\")\n",
        "              return \"추천 이유 생성 실패\"\n",
        "\n",
        "          return text\n",
        "      except requests.exceptions.Timeout:\n",
        "        print(\"LLM 호출 타임아웃\")\n",
        "        return \"타임아웃 오류\"\n",
        "      except requests.exceptions.HTTPError as e:\n",
        "          print(f\"HTTP 에러: {e.response.status_code}\")\n",
        "          return \"HTTP 오류\"\n",
        "      except Exception as e:\n",
        "          print(f\"LLM 호출 오류: {type(e).__name__}: {e}\")\n",
        "          return \"추천 이유 생성 실패\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3uh8vKS6KRA"
      },
      "outputs": [],
      "source": [
        "korean_stopwords = set([\n",
        "    '의', '가', '이', '은', '는', '을', '를', '에', '으로', '와', '과',\n",
        "    '도', '에서', '하다', '이다', '있다', '되다', '및', '하지만', '또한',\n",
        "    '그러나', '때문에', '그', '저', '등', '연구', '데이터', '방법'\n",
        "])\n",
        "\n",
        "\n",
        "# NLTK 기본 불용어\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# 논문 특화 불용어\n",
        "paper_stopwords = ['study', 'research', 'result', 'results', 'method', 'methods',\n",
        "    'data', 'analysis', 'based', 'using', 'approach', 'show', 'shown',\n",
        "    'paper', 'model', 'models', 'effect', 'effects', 'provide', 'at']\n",
        "\n",
        "all_english_stopwords = english_stopwords.union(set(paper_stopwords))\n",
        "\n",
        "def clean_text(text, okt=None):\n",
        "    \"\"\"\n",
        "    한국어 + 영어 불용어 제거\n",
        "    okt: KoNLPy 형태소 분석기 (선택)\n",
        "    \"\"\"\n",
        "    words = text.split()  # 간단히 공백 기준 토큰화\n",
        "    words = [w for w in words if w.lower() not in all_english_stopwords and w not in korean_stopwords]\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP3WX-c06MVM"
      },
      "outputs": [],
      "source": [
        "# 유클리디안 유사도 계산\n",
        "def euclidean_similarity(vec_a, vec_b):\n",
        "    return 1 / (1 + euclidean_distances(vec_a, vec_b).flatten())\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "def cosine_similarity_score(vec_a, vec_b):\n",
        "    return cosine_similarity(vec_a, vec_b).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZN0VgDz6N-Y"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(texts, embed_model, normalize=True):\n",
        "    \"\"\"문서 리스트를 임베딩\"\"\"\n",
        "    return embed_model.encode(texts, show_progress_bar=False, normalize_embeddings=normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgA9psx86A0w"
      },
      "outputs": [],
      "source": [
        "# 토픽 추출\n",
        "def extract_topics(texts, embeddings, num_topics=5):\n",
        "    topic_model = BERTopic(nr_topics=num_topics, calculate_probabilities=False)\n",
        "    topics, probs = topic_model.fit_transform(texts, embeddings=embeddings)\n",
        "\n",
        "    topic_vectors = {}\n",
        "    for t in set(topics):\n",
        "        if t == -1:\n",
        "            continue\n",
        "        idx = [i for i, top in enumerate(topics) if top == t]\n",
        "        topic_vectors[t] = np.mean(embeddings[idx], axis=0)\n",
        "    return topic_model, topics, topic_vectors\n",
        "\n",
        "# 토픽 요약\n",
        "def summarize_topics(topic_model):\n",
        "    topic_info = topic_model.get_topics()\n",
        "    summaries = []\n",
        "    for topic_num, words in topic_info.items():\n",
        "        if topic_num == -1:\n",
        "            continue\n",
        "        filtered_words = [w for w, _ in words if w.lower() not in all_english_stopwords and w not in korean_stopwords]\n",
        "        if filtered_words:\n",
        "            summaries.append(f\"Topic {topic_num}: {', '.join(filtered_words[:5])}\")\n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKVPPqtUNmgC"
      },
      "outputs": [],
      "source": [
        "#정규화 함수\n",
        "def normalize(arr):\n",
        "    arr = np.array(arr)\n",
        "    return (arr - arr.min()) / (arr.max() - arr.min() + 1e-9)\n",
        "\n",
        "# 토픽+ 코사인 + 유클리디안\n",
        "def recommend_coeu_with_llm(reference_docs, candidate_docs, embed_model,\n",
        "                            okt=None, url=None, headers=None,\n",
        "                            num_topics=5, alpha=0.6, top_k=5,\n",
        "                            weights=(0.2, 0.4, 0.4), use_llm=True):\n",
        "    \"\"\"\n",
        "    Coeu 기반 추천 함수\n",
        "    (Topic + Cosine + Euclidean 유사도 조합)\n",
        "    \"\"\"\n",
        "    # --- 후보 필터링 ---\n",
        "    candidate_docs = [c for c in candidate_docs\n",
        "                      if c.get('title', '').strip() != reference_docs[0].get('title', '').strip()]\n",
        "    if not candidate_docs:\n",
        "        return [], []\n",
        "\n",
        "    # --- 텍스트 정리 ---\n",
        "    ref_texts = [clean_text(r.get('title', '') + \" \" + r.get('description', ''), okt)\n",
        "                 for r in reference_docs]\n",
        "    cand_texts = [clean_text(c.get('title', '') + \" \" + c.get('description', ''), okt)\n",
        "                  for c in candidate_docs]\n",
        "\n",
        "    # --- 임베딩 계산 ---\n",
        "    all_texts = (ref_texts + cand_texts) * 10\n",
        "    emb = get_embeddings(all_texts, embed_model)\n",
        "\n",
        "    # --- 토픽 추출 ---\n",
        "    topic_model, topics, topic_vectors = extract_topics(all_texts, emb, num_topics=num_topics)\n",
        "    doc_topics = topics[:len(all_texts)//10]\n",
        "\n",
        "    # --- 벡터 준비 ---\n",
        "    ref_vec = np.mean([topic_vectors.get(t, np.zeros(emb.shape[1]))\n",
        "                       for t in doc_topics[:len(ref_texts)]], axis=0)\n",
        "    cand_vecs = np.array([topic_vectors.get(t, np.zeros(emb.shape[1]))\n",
        "                          for t in doc_topics[len(ref_texts):]])\n",
        "\n",
        "    ref_emb = emb[:len(ref_texts)]\n",
        "    cand_emb = emb[len(ref_texts):len(ref_texts)+len(cand_texts)]\n",
        "\n",
        "    # --- 유사도 계산 ---\n",
        "    topic_sim   = cosine_similarity_score(cand_vecs, ref_vec.reshape(1, -1))\n",
        "    text_co_sim = cosine_similarity_score(cand_emb, np.mean(ref_emb, axis=0).reshape(1, -1))\n",
        "    text_eu_sim = euclidean_similarity(cand_emb, np.mean(ref_emb, axis=0).reshape(1, -1))\n",
        "\n",
        "    # --- 유사도 정규화 ---\n",
        "    topic_sim   = normalize(topic_sim)\n",
        "    text_co_sim = normalize(text_co_sim)\n",
        "    text_eu_sim = normalize(text_eu_sim)\n",
        "\n",
        "    # --- 점수 계산 ---\n",
        "    w_t, w_c, w_e = weights\n",
        "    score = w_t * topic_sim + w_c * text_co_sim + w_e * text_eu_sim\n",
        "    top_idx = score.argsort()[-top_k:][::-1]\n",
        "\n",
        "    # --- 결과 구성 ---\n",
        "    results = []\n",
        "    for rank, i in enumerate(top_idx, 1):\n",
        "        reason = generate_reason_llm(reference_docs[0], candidate_docs[i], score[i], url, headers) \\\n",
        "                 if use_llm else \"LLM skipped\"\n",
        "        results.append({\n",
        "            \"rank\": rank,\n",
        "            \"division\": candidate_docs[i].get('division', ''),\n",
        "            \"title\": candidate_docs[i].get('title', ''),\n",
        "            \"score\": round(float(score[i]), 3),\n",
        "            \"reason\": reason,\n",
        "            \"url\": candidate_docs[i].get('url', '')\n",
        "        })\n",
        "\n",
        "    return results, summarize_topics(topic_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga-9waUVNf_G"
      },
      "outputs": [],
      "source": [
        "# 가중치 그리드서치 함수\n",
        "def grid_search_weights(reference_docs, candidate_docs, embed_model, okt=None,url=None, headers=None,\n",
        "                        num_topics=5, top_k=5, use_llm=False,\n",
        "                        weight_steps=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)):\n",
        "    \"\"\"\n",
        "    여러 (w_t, w_c, w_e) 조합을 돌려 평균 점수가 가장 높은 가중치를 리턴하는 함수\n",
        "    \"\"\"\n",
        "    best_score = -1\n",
        "    best_weights = (0.2, 0.4, 0.4)  # 초기 기본값\n",
        "\n",
        "    for w_t, w_c, w_e in product(weight_steps, repeat=3):\n",
        "        if abs((w_t + w_c + w_e) - 1.0) > 1e-6:\n",
        "            continue\n",
        "\n",
        "        results, _ = recommend_coeu_with_llm(\n",
        "            reference_docs, candidate_docs, embed_model,\n",
        "            okt=okt,url=url, headers=headers, num_topics=num_topics, top_k=top_k,\n",
        "            weights=(w_t, w_c, w_e), use_llm=use_llm\n",
        "        )\n",
        "\n",
        "        if not results:\n",
        "            continue\n",
        "\n",
        "        avg_score = float(np.mean([r[\"score\"] for r in results]))\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_weights = (w_t, w_c, w_e)\n",
        "\n",
        "    print(f\"[GridSearch] 최적 가중치={best_weights}, 평균 점수={round(best_score,3)}\")\n",
        "    return best_weights  # 가중치 3개만 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nz9_OmyRYBu"
      },
      "outputs": [],
      "source": [
        "# === 최종 리포트 표 생성 ===\n",
        "def make_final_report(final_results, candidate_docs, desc_maxlen=300):\n",
        "    # 후보 메타데이터(설명/URL) 조회용 인덱스\n",
        "    by_title = {}\n",
        "    for c in candidate_docs:\n",
        "        t = (c.get('title','') or '').strip()\n",
        "        if t:  # 중복 타이틀이 있을 수 있어 첫 항목 우선\n",
        "            by_title.setdefault(t, c)\n",
        "\n",
        "    rows = []\n",
        "    for r in final_results:\n",
        "        title = (r.get('title','') or '').strip()\n",
        "        meta  = by_title.get(title, {})\n",
        "        desc  = (meta.get('description','') or '').strip()\n",
        "        url   = (meta.get('url') or meta.get('link') or meta.get('pdf_url') or meta.get('doi') or '').strip()\n",
        "        reason = r.get('reason','')\n",
        "        if 'LLM' in reason and 'skipped' in reason.lower():\n",
        "            reason = ''\n",
        "\n",
        "        score = float(r.get('score', 0.0))\n",
        "        if score >= 0.8:\n",
        "            recommendation_level = \"강추\"\n",
        "        elif score >= 0.5:\n",
        "            recommendation_level = \"추천\"\n",
        "        elif score >= 0.3:\n",
        "            recommendation_level = \"참고\"\n",
        "        else:\n",
        "            recommendation_level = \"관련없음\"\n",
        "\n",
        "        rows.append({\n",
        "            \"구분\": r.get('division') or meta.get('division',''),\n",
        "            \"제목\": title,\n",
        "            #\"설명\": desc[:desc_maxlen],\n",
        "            \"점수\": score,\n",
        "            \"추천수준\": recommendation_level,\n",
        "            \"추천사유\": reason,\n",
        "            \"URL\": url\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    # 점수 내림차순 정렬\n",
        "    if not df.empty:\n",
        "        df = df.sort_values(\"점수\", ascending=False, kind=\"mergesort\").reset_index(drop=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 성능 지표"
      ],
      "metadata": {
        "id": "tp3XtZ1TBxiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nDCG@K 계산\n",
        "def ndcg_at_k(id: str, recommended: pd.DataFrame, test_data: pd.DataFrame, k: int = 5) -> float:\n",
        "    \"\"\"\n",
        "    NDCG@k 계산\n",
        "\n",
        "    Parameters:\n",
        "    - id: 추천 기준 데이터의 id (인덱스)\n",
        "    - recommended: 추천 결과 DataFrame (index: id, columns: '결과')\n",
        "    - test_data: 테스트 데이터 DataFrame (columns: '입력id', '결과')\n",
        "    - k: top-k\n",
        "\n",
        "    Returns:\n",
        "    - NDCG@k 평균값\n",
        "    \"\"\"\n",
        "    # 해당 id의 테스트 데이터 가져오기\n",
        "    test_results = test_data[test_data['입력id'] == id]\n",
        "    if len(test_results) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # 해당 id의 추천 결과 가져오기\n",
        "    if id not in recommended.index:\n",
        "        return 0.0\n",
        "\n",
        "    rec_results = recommended.loc[id, '결과'].tolist()[:k]\n",
        "    true_results = set(test_results['결과'].tolist())\n",
        "\n",
        "    # DCG 계산\n",
        "    dcg = 0\n",
        "    for i, result in enumerate(rec_results):\n",
        "        if result in true_results:\n",
        "            dcg += 1 / np.log2(i + 2)  # rank는 1부터 시작하므로 i+2\n",
        "\n",
        "    # IDCG 계산 (이상적인 경우: 첫 번째부터 정답이 나올 때)\n",
        "    idcg = sum([1 / np.log2(j + 2) for j in range(min(len(true_results), k))])\n",
        "\n",
        "    # nDCG 계산\n",
        "    ndcg = dcg / idcg if idcg > 0 else 0\n",
        "    return ndcg\n",
        "\n",
        "\n",
        "# Recall@K 계산\n",
        "def recall_at_k(id: str, recommended: pd.DataFrame, test_data: pd.DataFrame, k: int = 5) -> float:\n",
        "    \"\"\"\n",
        "    Recall@k 계산\n",
        "\n",
        "    Parameters:\n",
        "    - id: 추천 기준 데이터의 id (인덱스)\n",
        "    - recommended: 추천 결과 DataFrame (index: id, columns: '결과')\n",
        "    - test_data: 테스트 데이터 DataFrame (columns: '입력id', '결과')\n",
        "    - k: top-k\n",
        "\n",
        "    Returns:\n",
        "    - Recall@k 값\n",
        "    \"\"\"\n",
        "    # 해당 id의 테스트 데이터 가져오기\n",
        "    test_results = test_data[test_data['입력id'] == id]\n",
        "    if len(test_results) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # 해당 id의 추천 결과 가져오기\n",
        "    if id not in recommended.index:\n",
        "        return 0.0\n",
        "\n",
        "    rec_results = set(recommended.loc[id, '결과'].tolist()[:k])\n",
        "    true_results = set(test_results['결과'].tolist())\n",
        "\n",
        "    # Recall 계산: 정답 중에 추천된 것의 비율\n",
        "    hits = len(rec_results & true_results)\n",
        "    recall = hits / len(true_results) if len(true_results) > 0 else 0\n",
        "\n",
        "    return recall"
      ],
      "metadata": {
        "id": "1T5pfoOf0BCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 함수 적용"
      ],
      "metadata": {
        "id": "yLfJ6YKBBtdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 ID 로드 → reference 한 건 수집\n",
        "test_id = pd.read_csv(\"/content/drive/MyDrive/BDA/Rec_Agent/Rec_Agenttest_id_list.csv\")"
      ],
      "metadata": {
        "id": "dtWWG8s96s2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8yQceBXNwYt",
        "outputId": "afe5c483-40b7-486d-c429-ae0ff4558d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한글 쿼리: ['데이터 통합 | 데이터 통합 기법 교차 학문 | 다학제적 과학 전사체 | 전사체 분석 클러스터 유사도 스펙트럼 | 클러스터 유사도 분석 단세포 유전체 | 단세포 유전체 데이터']\n",
            "영어 쿼리: ['data integration | data integration methods interdisciplinary sciences | cross-disciplinary research transcriptome | transcriptome analysis cluster similarity spectrum | cluster similarity analysis single cell genomics | single cell genomics data']\n",
            "제한 적용 후 한글 쿼리: ['데이터 통합 | 데이터 통합 기법 교차 학문 | 다학제적 과학 전사체 | 전사체 분석 클러스터 유사도 스펙트럼 | 클러스터 유사도 분석 단세포 유전체 | 단세포 유전체 데이터']\n",
            "제한 적용 후 영어 쿼리: ['data integration | data integration methods interdisciplinary sciences | cross-disciplinary research transcriptome | transcriptome analysis cluster similarity spectrum | cluster similarity analysis single cell genomics | single cell genomics data']\n",
            "No records found for '데이터 통합 | 데이터 통합 기법 교차 학문 | 다학제적 과학 전사체 | 전사체 분석 클러스터 유사도 스펙트럼 | 클러스터 유사도 분석 단세포 유전체 | 단세포 유전체 데이터'\n",
            "'데이터 통합 | 데이터 통합 기법 교차 학문 | 다학제적 과학 전사체 | 전사체 분석 클러스터 유사도 스펙트럼 | 클러스터 유사도 분석 단세포 유전체 | 단세포 유전체 데이터' 검색 결과가 없습니다.\n",
            "No records found for 'data integration | data integration methods interdisciplinary sciences | cross-disciplinary research transcriptome | transcriptome analysis cluster similarity spectrum | cluster similarity analysis single cell genomics | single cell genomics data'\n",
            "'data integration | data integration methods interdisciplinary sciences | cross-disciplinary research transcriptome | transcriptome analysis cluster similarity spectrum | cluster similarity analysis single cell genomics | single cell genomics data' 검색 결과가 없습니다.\n",
            "'데이터 통합 | 데이터 통합 기법 교차 학문 | 다학제적 과학 전사체 | 전사체 분석 클러스터 유사도 스펙트럼 | 클러스터 유사도 분석 단세포 유전체 | 단세포 유전체 데이터' 검색 결과가 없습니다.\n",
            "'data integration | data integration methods interdisciplinary sciences | cross-disciplinary research transcriptome | transcriptome analysis cluster similarity spectrum | cluster similarity analysis single cell genomics | single cell genomics data' 검색 결과가 없습니다.\n",
            "candidates가 비었습니다.\n",
            "한글 쿼리: ['(데이터 통합 | 데이터 통합 기술) (교차학문 | 다학문 연구) (전사체 | 전사체 분석 | RNA 시퀀싱) 기타']\n",
            "영어 쿼리: ['(data integration | data integration techniques) (interdisciplinary sciences | cross-disciplinary research) (transcriptome | transcriptomics | RNA sequencing) other']\n",
            "제한 적용 후 한글 쿼리: ['(데이터 통합 | 데이터 통합 기술) AND (교차학문 | 다학문 연구) AND (전사체 | 전사체 분석 | RNA 시퀀싱) 기타']\n",
            "제한 적용 후 영어 쿼리: ['(data integration | data integration techniques) AND (interdisciplinary sciences | cross-disciplinary research) AND (transcriptome | transcriptomics | RNA sequencing) other']\n",
            "No records found for '(데이터 통합 | 데이터 통합 기술) AND (교차학문 | 다학문 연구) AND (전사체 | 전사체 분석 | RNA 시퀀싱) 기타'\n",
            "'(데이터 통합 | 데이터 통합 기술) AND (교차학문 | 다학문 연구) AND (전사체 | 전사체 분석 | RNA 시퀀싱) 기타' 검색 결과가 없습니다.\n",
            "No records found for '(data integration | data integration techniques) AND (interdisciplinary sciences | cross-disciplinary research) AND (transcriptome | transcriptomics | RNA sequencing) other'\n",
            "'(data integration | data integration techniques) AND (interdisciplinary sciences | cross-disciplinary research) AND (transcriptome | transcriptomics | RNA sequencing) other' 검색 결과가 없습니다.\n",
            "'(data integration | data integration techniques) AND (interdisciplinary sciences | cross-disciplinary research) AND (transcriptome | transcriptomics | RNA sequencing) other' 검색 결과가 없습니다.\n",
            "candidates 수집 완료: 1개\n",
            "=== FINAL (Top-K) ===\n",
            "{'rank': 1, 'division': 'paper', 'title': 'Advanced Monitoring and Reduction of Bioaerosols Leveraging DNA Sampling, Machine Learning Models, and Metal Oxide Filtration for Improved Air Quality', 'score': 0.0, 'reason': '추천 이유: 주제 유사성은 두 문서가 각각 단일 세포 유전체 데이터 통합과 바이오에어로졸 모니터링이라는 본질적으로 다른 연구 주제를 다루고 있어 명확히 차별화된다. 연구 방법 유사성은 하나가 클러스터 유사도 스펙트럼 기반의 무지도출 알고리즘을 활용하는 반면, 다른 문서는 DNA 분석과 머신러닝 모델을 결합한 환경 모니터링 기법을 사용하여 방법론적 공통점이 없다. 적용 분야 유사성은 첫 번째 문서가 생물학적 데이터 통합을 위한 컴퓨팅 기법을, 두 번째 문서가 대기 질 개선을 위한 환경 공학 기법을 각각 탐구하고 있어 활용 영역에서 중복이 발생하지 않는다.', 'url': ''}\n",
            "=== TOPIC SUMMARY ===\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# ✅ 준비: 임베딩/형태소기, 레퍼런스 & 후보 만들기\n",
        "# -----------------------------\n",
        "embed_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "okt = Okt()\n",
        "\n",
        "while True:\n",
        "    # 데이터 수집 및 쿼리 생성\n",
        "    reference = collect_data(test_id.iloc[1].values, \"DataOn\", row_count=1, return_type=\"dict\")\n",
        "\n",
        "    ko_query, en_query = generate_query(reference, url, headers)\n",
        "\n",
        "    max_attempts = 5\n",
        "    attempt = 0\n",
        "    while (len(ko_query) == 0 or len(en_query) == 0) and attempt < max_attempts:\n",
        "        ko_query, en_query = generate_query(reference, url, headers)\n",
        "        attempt += 1\n",
        "\n",
        "    print(f\"한글 쿼리: {ko_query}\")\n",
        "    print(f\"영어 쿼리: {en_query}\")\n",
        "\n",
        "    ko_query_limit = [limit_boolean_query(q) for q in ko_query]\n",
        "    en_query_limit = [limit_boolean_query(q) for q in en_query]\n",
        "\n",
        "    print(f\"제한 적용 후 한글 쿼리: {ko_query_limit}\")\n",
        "    print(f\"제한 적용 후 영어 쿼리: {en_query_limit}\")\n",
        "\n",
        "    dataon_ko_docs    = collect_data(ko_query_limit[0], \"DataOn\",    row_count=50, return_type=\"dict\")\n",
        "    dataon_en_docs    = collect_data(en_query_limit[0], \"DataOn\",    row_count=50, return_type=\"dict\")\n",
        "    scienceon_ko_docs = collect_data(ko_query_limit[0], \"ScienceOn\", row_count=50, return_type=\"dict\")\n",
        "    scienceon_en_docs = collect_data(en_query_limit[0], \"ScienceOn\", row_count=50, return_type=\"dict\")\n",
        "\n",
        "    candidates = dataon_ko_docs + dataon_en_docs + scienceon_ko_docs + scienceon_en_docs\n",
        "\n",
        "    if len(candidates) > 0:\n",
        "        print(f\"candidates 수집 완료: {len(candidates)}개\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"candidates가 비었습니다.\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ✅ 1) 그리드서치로 최적 가중치 찾기 (탐색 중 LLM 비활성)\n",
        "# -----------------------------\n",
        "best_weights = grid_search_weights(\n",
        "    reference, candidates, embed_model, okt=okt,url=url, headers=headers,\n",
        "    num_topics=5, top_k=5, use_llm=False,\n",
        "    weight_steps=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\n",
        ")\n",
        "print(\">> BEST WEIGHTS =\", best_weights)\n",
        "\n",
        "# -----------------------------\n",
        "# ✅ 2) 베이스라인(0.2,0.4,0.4) vs 최적 가중치 성능 비교\n",
        "#    - 평균 점수(Top-K의 score 평균)로 간단 비교\n",
        "# -----------------------------\n",
        "def _avg_score(weights, url=url, headers=headers,):\n",
        "    res, _ = recommend_coeu_with_llm(\n",
        "        reference, candidates, embed_model, okt=okt,url=url, headers=headers,\n",
        "        num_topics=5, top_k=5, weights=weights, use_llm=False\n",
        "    )\n",
        "    return float(np.mean([r[\"score\"] for r in res])) if res else -1.0\n",
        "\n",
        "baseline_w = (0.2, 0.4, 0.4)\n",
        "baseline_avg = _avg_score(baseline_w, url=url, headers=headers)\n",
        "best_avg     = _avg_score(best_weights, url=url, headers=headers)\n",
        "\n",
        "print(f\"[COMPARE] baseline {baseline_w} avg={baseline_avg:.3f}  vs  best {best_weights} avg={best_avg:.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# ✅ 3) 최적 가중치로 최종 추천 (이때만 LLM 이유 생성 ON)\n",
        "# -----------------------------\n",
        "\n",
        "final_results, topics = recommend_coeu_with_llm(\n",
        "    reference, candidates, embed_model, okt=okt,url=url, headers=headers,\n",
        "    num_topics=5, top_k=5, weights=best_weights, use_llm=True\n",
        ")\n",
        "\n",
        "print(\"=== FINAL (Top-K) ===\")\n",
        "for r in final_results:\n",
        "    print(r)\n",
        "print(\"=== TOPIC SUMMARY ===\")\n",
        "for t in topics:\n",
        "    print(\"-\", t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kZL22fD8EXV",
        "outputId": "53dfe278-7caa-46a1-d522-871974d50ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   구분                                                                                                                                                     제목  점수 추천수준                                                                                                                                                                                                                                                                                                                     추천사유 URL\n",
            "paper Advanced Monitoring and Reduction of Bioaerosols Leveraging DNA Sampling, Machine Learning Models, and Metal Oxide Filtration for Improved Air Quality 0.0 관련없음 추천 이유: 주제 유사성은 두 문서가 각각 단일 세포 유전체 데이터 통합과 바이오에어로졸 모니터링이라는 본질적으로 다른 연구 주제를 다루고 있어 명확히 차별화된다. 연구 방법 유사성은 하나가 클러스터 유사도 스펙트럼 기반의 무지도출 알고리즘을 활용하는 반면, 다른 문서는 DNA 분석과 머신러닝 모델을 결합한 환경 모니터링 기법을 사용하여 방법론적 공통점이 없다. 적용 분야 유사성은 첫 번째 문서가 생물학적 데이터 통합을 위한 컴퓨팅 기법을, 두 번째 문서가 대기 질 개선을 위한 환경 공학 기법을 각각 탐구하고 있어 활용 영역에서 중복이 발생하지 않는다.    \n"
          ]
        }
      ],
      "source": [
        "report_df = make_final_report(final_results, candidates, desc_maxlen=300)\n",
        "print(report_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(\"/content/drive/MyDrive/BDA/Rec_Agent/data/final_test_data.csv\")# 테스트 데이터 로드\n",
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "A4xlJ8xs4K3P",
        "outputId": "fca4a079-188c-4136-a4a1-084820d3fcbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 입력id  \\\n",
              "0    66dd38ee92a4be4030294ea13691fd1e   \n",
              "1    66dd38ee92a4be4030294ea13691fd1e   \n",
              "2    66dd38ee92a4be4030294ea13691fd1e   \n",
              "3    66dd38ee92a4be4030294ea13691fd1e   \n",
              "4    66dd38ee92a4be4030294ea13691fd1e   \n",
              "..                                ...   \n",
              "176  d790111eaaa8cad3480674fcca0f2a5e   \n",
              "177  d790111eaaa8cad3480674fcca0f2a5e   \n",
              "178  d790111eaaa8cad3480674fcca0f2a5e   \n",
              "179  d790111eaaa8cad3480674fcca0f2a5e   \n",
              "180  d790111eaaa8cad3480674fcca0f2a5e   \n",
              "\n",
              "                                                    입력  \\\n",
              "0    Relative abundances of archaeal amplicon seque...   \n",
              "1    Relative abundances of archaeal amplicon seque...   \n",
              "2    Relative abundances of archaeal amplicon seque...   \n",
              "3    Relative abundances of archaeal amplicon seque...   \n",
              "4    Relative abundances of archaeal amplicon seque...   \n",
              "..                                                 ...   \n",
              "176  GEO Mountains Inventory of In Situ Observation...   \n",
              "177  GEO Mountains Inventory of In Situ Observation...   \n",
              "178  GEO Mountains Inventory of In Situ Observation...   \n",
              "179  GEO Mountains Inventory of In Situ Observation...   \n",
              "180  GEO Mountains Inventory of In Situ Observation...   \n",
              "\n",
              "                                                  추천결과       구분 추천수준  \n",
              "0    Nucleotide sequences of archaeal amplicon sequ...  dataset   참고  \n",
              "1    From Recharge, to Groundwater, to Discharge Ar...    paper   참고  \n",
              "2    Hydrogen and dark oxygen drive microbial produ...    paper   추천  \n",
              "3    Linking Groundwater to Surface Discharge Ecosy...    paper   참고  \n",
              "4    Energy efficiency and biological interactions ...    paper   참고  \n",
              "..                                                 ...      ...  ...  \n",
              "176  Coverage of In Situ Climatological Observation...    paper   참고  \n",
              "177  Advancing Observational Infrastructure in the ...    paper   참고  \n",
              "178  Environmental monitoring in Jiangsu Province: ...    paper   참고  \n",
              "179  Development of Networked Environmental Monitor...    paper   참고  \n",
              "180  Environmental Monitoring System for Base Stati...    paper   참고  \n",
              "\n",
              "[181 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c32b828-a97f-4197-9ea5-0e2761fec6cc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>입력id</th>\n",
              "      <th>입력</th>\n",
              "      <th>추천결과</th>\n",
              "      <th>구분</th>\n",
              "      <th>추천수준</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>66dd38ee92a4be4030294ea13691fd1e</td>\n",
              "      <td>Relative abundances of archaeal amplicon seque...</td>\n",
              "      <td>Nucleotide sequences of archaeal amplicon sequ...</td>\n",
              "      <td>dataset</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>66dd38ee92a4be4030294ea13691fd1e</td>\n",
              "      <td>Relative abundances of archaeal amplicon seque...</td>\n",
              "      <td>From Recharge, to Groundwater, to Discharge Ar...</td>\n",
              "      <td>paper</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66dd38ee92a4be4030294ea13691fd1e</td>\n",
              "      <td>Relative abundances of archaeal amplicon seque...</td>\n",
              "      <td>Hydrogen and dark oxygen drive microbial produ...</td>\n",
              "      <td>paper</td>\n",
              "      <td>추천</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>66dd38ee92a4be4030294ea13691fd1e</td>\n",
              "      <td>Relative abundances of archaeal amplicon seque...</td>\n",
              "      <td>Linking Groundwater to Surface Discharge Ecosy...</td>\n",
              "      <td>paper</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>66dd38ee92a4be4030294ea13691fd1e</td>\n",
              "      <td>Relative abundances of archaeal amplicon seque...</td>\n",
              "      <td>Energy efficiency and biological interactions ...</td>\n",
              "      <td>paper</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>d790111eaaa8cad3480674fcca0f2a5e</td>\n",
              "      <td>GEO Mountains Inventory of In Situ Observation...</td>\n",
              "      <td>Coverage of In Situ Climatological Observation...</td>\n",
              "      <td>paper</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>d790111eaaa8cad3480674fcca0f2a5e</td>\n",
              "      <td>GEO Mountains Inventory of In Situ Observation...</td>\n",
              "      <td>Advancing Observational Infrastructure in the ...</td>\n",
              "      <td>paper</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>d790111eaaa8cad3480674fcca0f2a5e</td>\n",
              "      <td>GEO Mountains Inventory of In Situ Observation...</td>\n",
              "      <td>Environmental monitoring in Jiangsu Province: ...</td>\n",
              "      <td>paper</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>d790111eaaa8cad3480674fcca0f2a5e</td>\n",
              "      <td>GEO Mountains Inventory of In Situ Observation...</td>\n",
              "      <td>Development of Networked Environmental Monitor...</td>\n",
              "      <td>paper</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>d790111eaaa8cad3480674fcca0f2a5e</td>\n",
              "      <td>GEO Mountains Inventory of In Situ Observation...</td>\n",
              "      <td>Environmental Monitoring System for Base Stati...</td>\n",
              "      <td>paper</td>\n",
              "      <td>참고</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>181 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c32b828-a97f-4197-9ea5-0e2761fec6cc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c32b828-a97f-4197-9ea5-0e2761fec6cc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c32b828-a97f-4197-9ea5-0e2761fec6cc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-92a54665-32ac-491e-aed5-41d0c106c3fd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-92a54665-32ac-491e-aed5-41d0c106c3fd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-92a54665-32ac-491e-aed5-41d0c106c3fd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_bf8164e6-452b-48bd-ab8c-c3248e36d860\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bf8164e6-452b-48bd-ab8c-c3248e36d860 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_data",
              "summary": "{\n  \"name\": \"test_data\",\n  \"rows\": 181,\n  \"fields\": [\n    {\n      \"column\": \"\\uc785\\ub825id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 27,\n        \"samples\": [\n          \"ac72f4f9b7730b9032d0b58347cbe468\",\n          \"33e8b6d95eedf4e611d778ba8827afa9\",\n          \"8061c93c2ec807e5868c8d6d303127de\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc785\\ub825\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"A Multi-Type\\u00a0Birth-Death\\u00a0model for Bayesian inference of lineage-specific\\u00a0birth\\u00a0and\\u00a0death\\u00a0rates\",\n          \"Educational\\u00a0Resources from UC Berkeley RDM Librarian Training\\u00a0Program\",\n          \"Relative abundances of archaeal amplicon sequence variants (ASV) from Canadian Prairie groundwater ecosystems\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ucd94\\ucc9c\\uacb0\\uacfc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 179,\n        \"samples\": [\n          \"Causal Relationship between Exports and Agricultural GDP in India\",\n          \"Tissue-specific gene regulatory networks inferred with ANANSE 0.2.0\",\n          \"\\ub7ec\\uc2dc\\uc544 \\uc774\\uc8fc\\uc758 \\ud604\\ud669\\uacfc \\ud2b9\\uc9d5 : \\uadf9\\ub3d9\\uc5f0\\ubc29\\uad6c\\uc5d0 \\ub300\\ud55c \\uacbd\\ud5d8\\uc801 \\ubd84\\uc11d (Migration Patterns in Russia : An Empirical Study in the Russian Far East)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uad6c\\ubd84\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"paper\",\n          \"dataset\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ucd94\\ucc9c\\uc218\\uc900\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"\\ucc38\\uace0\",\n          \"\\ucd94\\ucc9c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===정량 평가 결과===\")\n",
        "print(\"NDCG@2:\", ndcg_at_k(str(test_id.iloc[1]), report_df, test_data, k=5))\n",
        "print(\"Recall@2:\", recall_at_k(str(test_id.iloc[1]), report_df, test_data, k=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpbyl2Fn0A-G",
        "outputId": "dbe152b6-636f-49e0-eb16-958bcf1c286f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===정량 평가 결과===\n",
            "NDCG@2: 0.0\n",
            "Recall@2: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLNrYXMRz-9_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}